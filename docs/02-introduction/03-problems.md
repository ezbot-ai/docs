---
sidebar_position: 3
sidebar_label: Problems w/ Experimentation
---

# Problems with Traditional Experimentation

Running A/B tests to improve your website or app's conversion rates can be a powerful tool, but it's not without its challenges. Let’s explore some of the common hurdles you might face when using traditional experimentation methods.

## It's Hard

You might think A/B testing is as simple as making a guess about what might work better on your website, trying it, and seeing if it improves a metric you care about. While that’s the basic idea, the reality is much more complex. To truly benefit from A/B testing, you need expertise in several areas:

- **Experimental Design:** Structuring your tests correctly is vital. This involves deciding what to test, how to test it, how to measure success, and how to predict the amount of traffic you need to detect a difference. Poorly designed experiments can lead to misleading results, wasting time and resources.

- **Programming:** This involves coding changes and variations on your website or app. You might want to tweak designs, change layouts, or try different feature configurations, all of which require a good grasp of web development.

- **Data Analysis and Statistics:** Once your test is running and you have results, the real work begins. You need to interpret the data and make informed decisions. This involves understanding user behavior and analyzing metrics such as how long visitors stay on your site, their click patterns, and conversion rates.

You also need to ensure that your results are statistically significant, which is crucial for making reliable decisions. This means you must measure whether the differences you observe are due to the changes you made or just random chance. For this, you need a solid understanding of complex statistical concepts such as p-values, confidence intervals, and statistical power.

Here’s a brief look at these concepts:

- **P-values:** This helps you understand the probability that the observed differences happened by chance. A low p-value indicates that the differences are likely due to your changes and are not due to random chance.
- **Confidence Intervals:** This provides a range within which the true effect size lies, giving you an idea of the reliability of your results.
- **Statistical Power:** This measures the likelihood that your test will detect a real effect when there is one. Higher power means a greater chance of detecting true differences.

:::danger

Without this statistical expertise, you could make decisions based on faulty conclusions, leading to ineffective changes, wasted resources, or worse: negative impacts on your business.
:::

## It's Slow

Detecting small differences between variations takes time and a lot of traffic. If your website or app doesn’t have millions of monthly users, it can take weeks or even months to get statistically significant results.

Additionally, the deeper down your funnel you want to run your test, the lower the traffic and the longer the expected experiment run-time. For instance, if you want to run a test on your checkout page and only 3% of your users visit that page, reaching the traffic necessary for statistical significance can be a major challenge.

Even with top-notch A/B testing programs, the success rate is only about 20%. This means that 80% of your experiments might not give you useful insights, and may only confuse you.

These high traffic requirements make it tough for smaller companies to use A/B testing effectively without a lot of traffic or very long experiment times.Plus, issues like sample ratio mismatch can invalidate your results, adding another layer of complexity.

:::tip

Here’s a little-known fact: achieving statistical significance for a small improvement, like a 5% lift, requires a lot of traffic. For example, to detect a 5% improvement at a 95% significance level with 80% power, you need around 120,000 users per variant or **240,000 users** for a two-tailed A/B test. If you want to detect even smaller improvements or run more variants, you need even more traffic.

:::

## It's Expensive

Running traditional A/B tests can be quite costly for a few key reasons:

- **Expertise Required:** A/B testing isn't just about setting up a test and waiting for results. It involves continuous effort and specialized skills. You need statistical experts to design experiments that yield reliable results, data analysts to interpret the findings accurately, and developers to implement the necessary changes. Hiring or contracting these experts can be expensive, especially for a small business.

- **Human Bottlenecks:** The process of designing, running, and analyzing experiments involves several stages where human intervention is necessary. Each stage can slow down the overall process, and any delays can be costly. Whether it's waiting for a developer to implement changes or for an analyst to interpret data, these human bottlenecks increase the time and cost involved in running effective A/B tests.

- **High Traffic Requirements:** Detecting statistically significant differences between variations requires a lot of traffic. If your website or app doesn't have millions of monthly users, it can take weeks or even months to gather enough data. This prolonged period of experimentation ties up resources and delays potential improvements, making the entire process more expensive. The deeper down your funnel you go, the fewer users you have, further extending the time and cost needed to achieve reliable results.

Understanding these challenges can help you plan better and consider whether traditional A/B testing is the right approach for your business. By recognizing the costs and complexities, you can make more informed decisions about how to optimize your website or app effectively.
